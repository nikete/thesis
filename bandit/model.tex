%!TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model}
\label{sec:noncompliance}

This section introduces a formal setting for bandit algorithms with noncompliance and introduces protocols that prescribe how to make use of compliance information. Before diving into the formalism, let us discuss informally how compliance information can be useful. 

First, suppose that the patient population is homogeneous in their response to the treatment, and that patients take the treatment with probability $p$ if prescribed and probability $1-p$ otherwise, where $p<0.5$. In this setting, it is clear that a bandit algorithm will learn faster by rewarding arms according to whether the treatment was \emph{taken} by the patient, rather than whether it was \emph{recommended} to the patient. 

As a second example, consider \emph{corrective compliance} where patients who benefit from a treatment are more likely to take it, since they have access to information that the algorithm does not. The algorithm clearly benefits by learning from the information expressed in the behavior of the patients. Learning from the treatment actually taken is therefore more efficient than learning from the algorithm's recommendations. Further examples are provided in section~\ref{sec:formal}.
%TODO maybe move these two paragraphs to the motivational intro further up...

%TODO: frame as a partial monitoring games, two armed equivalence ref %http://arxiv.org/pdf/1108.4961v1.pdf 
% Toward a classification of finite partial-monitoring games
% Partial-monitoring games constitute a mathematical framework for sequential decision making problems with imperfect feedback: the learner repeatedly chooses an action, the opponent responds with an outcome, and then the learner suffers a loss and receives a feedback signal, both of which are fixed functions of the action and the outcome. The goal of the learner is to minimize his total cumulative loss. We make progress toward the classification of these games based on their minimax expected regret. Namely, we classify almost all games with two outcomes and a finite number of actions: we show that their minimax expected regret is either zero, , Θ(T2/3), or Θ(T), and we give a simple and efficiently computable classification of these four classes of games. Our hope is that the result can serve as a stepping stone toward classifying all finite partial-monitoring games.


\begin{figure*}[t]
	\centering	
	\include{BCADAG}
	\caption{Bandit with Compliance Awareness DAG}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Formal setting}
\label{sec:formal}

We consider a sequential decision making problem where a process mediates between the actions chosen by the algorithm and the action carried out in the world. Let $\cA=[k]=\{1,\ldots,k\}$ be the set of possible actions, and let $T$ be the number of observed time steps. The general game is as follows:

\begin{defn}[bandit with compliance information]\label{def:compliance_bandit}\eod
	At each time step $t\in[T]$, the player selects an action $c^{(t)}\in \cA$ (the chosen action). The environment responds by carrying out an action $a^{(t)}\in\cA$ (the actual action) and providing reward $r^{(t)}\in[0,1]$.
	%TODO decide on whether to use reward or loss...and do so coherently.
\end{defn}
The standard bandit setting is when $a^{(t)}$ is either unobserved or $a^{(t)}=c^{(t)}$ for all $t\in[T]$.

%Let $A$ be the random variable describing the action that is carried out, $C$ the random variable describing the choice taken by the subject, and let $U$ be the latent variables. We define the ``noncompliance level'' $n(c, u)$ for a specific choice $c$ and latent variable value $u$ mean the probability that $A \neq c$ given those values, that is, $n(c, u) = 1 - P(A=c|C=c, U=u)$.
%TODO you never use the noncompliance level again in this thesis!?

The set of compliance behaviors is the set of functions $\cC=\{\nu:\cA\rightarrow\cA\}$ from chosen to taken action.% \cite{koller:09}. 

\begin{defn}[model assumptions]\label{def:assumptions}\eod
	We make the following assumptions:
	\begin{enumerate}
		\item Compliance $\nu \in\cC$ depends on a latent variable sampled i.i.d. for each time step from unknown distribution $\bP(U)$ over a set $U$. Denote by $\nu_u$ the compliance behaviour under a given instance $u$ of the latent variable. 
		\item Outcomes $r(a,u)$ depend on treatment taken and the latent $u$, a fixed function $r: \cA\times U\rightarrow[0,1]$. In other words, the chosen action of the algorithm can only affect the reward received by affecting the actual action taken. 
	\end{enumerate}
\end{defn}

When $|\cA|=k=2$ (e.g., control and treatment), we can list the compliance-behaviors explicitly.
\begin{defn}[compliance behaviors]\label{def:compliance_model}\eod
	For $k=2$, the following four subpopulations capture all deterministic compliance-behaviors:
	\begin{itemize}
		\item never-takers $\fN: \Big(0\mapsto 0, 1\mapsto 0\Big)$
		\item always-takers $\fA: \Big(0\mapsto 1, 1\mapsto 1\Big)$
		\item compliers $\fC: \Big( 0\mapsto 0, 1\mapsto 1\Big)$
		\item defiers $\fD: \Big(0\mapsto 1, 1\mapsto 0\Big)$
	\end{itemize}
%	Let $p_s:= \expec_{u\sim \bP(U)}[\indic_{[\nu(u)=s]}]$ denote the probability of sampling from subpopulation $s\in\{\fN,\fA,\fC, \fD\}$.
   %TODO also never used i think...
\end{defn}

Unfortunately, the subpopulations cannot be distinguished from observations. For example, a patient that takes a prescribed treatment may be a complier or an always-taker. Nevertheless, observing compliance-behavior provides potentially useful side-information. The setting can be contrasted from contextual bandits because the side-information is only available \emph{after} the bandit algorithm chooses an arm.
%TODO why do you say this here?

\begin{defn}[stochastic reward model]\label{def:reward_model}\eod
	The expected reward given subpopulation $s$ and the actual treatment $a\in\cA$ is
	\begin{equation}
		r_{s,a} 
		:= \expec_{u\sim \bP(U)}\big[r(a,u)\,\big|\, \nu_u=s\big]
		\quad\text{for }s\in \{\fN, \fA,\fC,\fD\}.
		\label{eq:exp_rew}		
	\end{equation}		
\end{defn}

The goal is to maximize the cumulative reward received, i.e. choose a sequence of actions $(c^{(t)})_{t\in [T]}$ that maximizes 

\begin{equation}
   \expec_{u\sim \bP(U)}\left[\sum_{t\in[T]}r(\nu_u(c^{(t)}), u)\right]
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
