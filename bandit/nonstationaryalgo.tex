%!TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Algorithms}
%\label{sec:algol}

\section{Worst Case Linear Regret in Non-Stationary setting to Compliance Unawareness}
%TODO what?

In the non-compliance setting there is additional information available to the algorithm. Ignoring the compliance-information (\chosen) reduces to the standard bandit setting. However, it should be possible to improve performance by taking advantage of observations about when treatments are \emph{actually} applied. Using compliance information is not trivial, since bandit algorithms that rely purely on treatments (\actual) or purely on compliance (\comply) can have linear regret.

This section proposes two hybrid algorithms that take advantage of compliance information, have bounded regret, and empirically outperform algorithms running the \chosen\, protocol.


Consider the regret not relative to a best fixed action as usual, but relative to an algorithm that has access to compliance information.
We show that this regret scales $O(T)$ in the non-stationary setting if the regime from which losses are drawn changes frequently enough. We also show that within each regime the compliance awareness helps converge faster, for example due to very high rates of noncompliance of subjects that don't have different underlying characteristics, making \actual\, perform well, or due to subjects having information that helps them switch to the best arm.

%\NDP{Formalize into a theorem, example/witness}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Hierarchical Algorithm}

A natural idea is to use the three protocols to train three experts and, simultaneously, learn which expert to apply. The resulting hierarchical bandit (specified in  \ref{alg:hier-exp}) integrates compliance-information in a way that ensures the algorithm (i) has no-regret, because one of the base algorithms uses \chosen, and therefore has no regret; and (ii) benefits from the compliance-information if it turns out to be useful.

The general construction is as follows. At the bottom-level are three bandit algorithms implementing the protocols \chosen, \actual\, and \comply. On the top-level is a fourth bandit algorithm whose arms are the three bottom-level algorithms. The top-level bandit learns which protocol is optimal. 
Note the top-level bandit is \emph{not} in an i.i.d. environment even when the external environment is i.i.d, since the low-level bandits are learning.
 
\begin{algorithm}
   \caption{\texttt{HierarchicalBandit (HB)}}
   \label{alg:hier-exp}
   \begin{algorithmic}   
   \STATE {\bfseries Input:}
   	 Bandits $\cB_i$ running \texttt{NoRegretAlgorithm} on \comply\, \chosen, and \actual\, for $i =\{1,2,3\}$ respectively, with arms corresponding to treatments
   	 \STATE {\bfseries Input:}
   	 Bandit $\cH$ running \texttt{NoRegretAlgorithm} compatible with adaptive environments, with arms corresponding to $\cB_i$ above
    \FOR{$t=1$ {\bfseries to} $T$}
	   \STATE Draw bandit $i^{(t)}\in\{1,2,3\}$ from $\cH$ and arm $j^{(t)}$ from $\cB_{i^{(t)}}$
	   \STATE Pull arm $j^{(t)}$ of $\cB_{i^{(t)}}$; observe loss $\ell=\ell_{i^{(t)},j^{(t)}}^{(t)}$; observe compliance
	\STATE Update $\cH$ with loss $\ell$ applied to bandit-arm $i^{(t)}$
	\IF{$i^{(t)}=1$}
	\STATE Update $\cB_1$ with loss $\ell$ applied to treatment-arm $j^{(t)}$
	\ENDIF
	\STATE Update $\cB_{2/3}$ with loss $\ell$ according to protocols \chosen\, and \actual\, respectively
   	\ENDFOR

       	\end{algorithmic}
\end{algorithm}          

\subsection{Regret analysis}

\begin{defn}[Regret]
   The \emph{regret} of an online learning algorithm \texttt{A} is
   $$\texttt{Regret}_\texttt{A}(T) = \sum_{t\in[T]}\ell_{j^{(t)}}^{(t)}  - \min_j\sum_{t\in[T]}\ell_j^{(t)}$$
   where $j^{(t)}$ is the action chosen by the algorithm in time step $t$, and $\min_j\sum_{t\in[T]}\ell_j^{(t)}$ is the minimal accumulated loss one can obtain when fixing an action and choosing that same fixed action each step.
\end{defn}
	

This section shows that constructing a hierarchical bandit with \texttt{Exp3} (Algorithm \ref{alg:exp3}) as the top-level bandit algorithm yields a no-regret algorithm. The result is straightforward; we include it for completeness. A similar result was shown in \cite{chang:05}. 

The \texttt{Exp3} Algorithm (\cite{auer:02b}) is a bandit algorithm whose worst case regret bound is robust to adaptive environments.

\begin{algorithm}
   \caption{\texttt{Exp3}}
   \label{alg:exp3}
   \begin{algorithmic}   
      \STATE {\bfseries Input:} $\gamma\in[0,1]$\\
   \STATE Initialize weight vector $w^{(1)}_{i}=1$ for $i\in[N]$ where $N$ is the number of arms;
   \FOR{$t=1$ {\bfseries to} $T$}
	   \STATE Define probabilities $x_i^{(t)} = (1-\gamma)\frac{w_i^{(t)}}{ \sum_j w_{j}^{(t)} } + \gamma\frac{1}{N}$
	   \STATE Draw an arm $i^{(t)} \sim\mathbf{x}^{(t)}$
	\STATE Pull arm $i^{(t)} $
	   \STATE Incur loss $\ell^{(t)}$
	   \STATE Update:
	   \begin{align}
		w^{(t+1)}_i & = \begin{cases}
		   w_{i}^{(t)} \cdot \exp(-\gamma \frac{\ell^{(t)}}{N\cdot x_{i^{(t)}}})
			 & \text{if } i=i^{(t)} \\
			w^{(t)}_{i} & \text{else}
		\end{cases}		
		\end{align}
    \ENDFOR
       	\end{algorithmic}
\end{algorithm}         
 
To obtain a hierarchical algorithm using \texttt{Exp3} as top level, we first construct a hierarchical version of \texttt{Hedge}, Algorithm~\ref{alg:meta-hedge}, which is applicable in the full-information setting. We then modify it using the principle from $\texttt{Exp3}$ to make it work for bandit feedback settings.

\texttt{Hedge} (\cite{chang:05}) is an algorithm with bounded regret in the expert setting.
On the bottom-level of our hierarchical version (Algorithm \ref{alg:meta-hedge}), there are $M$ instantiations of \texttt{Hedge}. Instantiation $i$, for $i\in[M]$, plays an $N$-dimensional weight vector and receives $N$-dimensional loss vector $\loss^{(t)}_{i}$ on round $t$. We impose the assumption that all instantiations play $N$-vectors for notational convenience. The top-level is another instantiation of \texttt{Hedge}, which plays a weighted combination of the bottom-level instantiations.

\begin{algorithm}
   \caption{\texttt{Hierarchical Hedge (HHedge)}}
   \label{alg:meta-hedge}
   \begin{algorithmic}   
   	\STATE {\bfseries Input:} $\eta,\rho>0$\\
   	\STATE $v^{(1)}_{i}=1$ for $i\in[M]$
   	\STATE $w^{(1)}_{i,j}=1$ for $(i,j)\in[M]\times[N]$
	\FOR{$t=1$ {\bfseries to} $T$}
	   \STATE Set $\x^{(t)} = \frac{\vt^{(t)}}{\sum_{i\in[M]} v^{(t)}_{i}}$
	   \STATE Set $\y^{(t)}_{i} = \frac{\wt^{(t)}_{i}}{\sum_{j\in[N]} w^{(t)}_{i,j}}$ for $i\in[M]$.
		\STATE Receive feedback $\loss^{(t)}\in [0,1]^{M\times N}$ %TODO TODO why do the lower levels get different feedback each?
		\STATE Incur loss $\sum_{i=1}^{M} x^{(t)}_{i}\cdot\sum_{j=1}^N\ell^{(t)}_{i,j}\cdot y^{(t)}_{i,j}$
		\STATE Update weights for all $i,j$:
		\begin{align}
			v^{(t+1)}_i & = v^{(t)}_{i}\cdot \exp\big(-\eta \sum_{j=1}^N\ell^{(t)}_{i,j}\cdot y^{(t)}_{i,j}\big)
			\\
			w^{(t+1)}_{i,j} & = w^{(t)}_{i,j}\cdot \exp\big(-\rho\cdot \ell^{(t)}_{i,j}\big)
		\end{align}
    	\ENDFOR
   	\end{algorithmic}
\end{algorithm}

We have the following lemma:

\begin{lem}\label{lem:meta-hedge}
	Introduce compound loss vectors $\tilde{\loss}^{(t)}$ with 
	$$\tilde{\ell}^{(t)}_i := \sum_{j=1}^N \ell^{(t)}_{i,j}\cdot y^{(t)}_{i,j}$$
	Then $\rho$ can be chosen in \texttt{HHedge} such that for all $i\in[M]$,
	\begin{equation}
		\sum_{t=1}^T \langle \x^{(t)},\tilde{\loss}^{(t)}\rangle 
		\leq  \sum_{t=1}^T \tilde{\ell}^{(t)}_i +
		 O(\sqrt{T \log M})
	\end{equation}
	Moreover, $\rho$ and $\eta$ can be chosen such that for all $i\in[M]$ and all $j\in[N]$,
	\begin{equation}
	   \sum_{t=1}^T \langle \x^{(t)},\tilde{\loss}^{(t)}\rangle 
		\leq \sum_{t=1}^T\ell^{(t)}_{i,j}
		+ O(\sqrt{T \log M} + \sqrt{T \log N}).
	\end{equation}
\end{lem}

Lemma~\ref{lem:meta-hedge} says, firstly, that \texttt{HHedge} has bounded regret relative to the bottom-level instantiations and, secondly, that it has bounded regret relative to any of the $M\times N$ experts on the bottom-level.


Algorithm~\ref{alg:meta-exp2} modifies \texttt{HHedge} so that it is suitable for bandit feedback, yielding \texttt{HExp3}. A corresponding no-regret bound follows immediately:

\begin{lem}\label{lem:meta-exp}
	Define $\tilde{\loss}^{(t)}$ as in Lemma~\ref{lem:meta-hedge} to obtain the expected loss for the upper-level \texttt{Exp3} instances:
	$$\tilde{\ell}^{(t)}_i := \sum_{j=1}^N \ell^{(t)}_{i,j}\cdot y^{(t)}_{i,j} = \expec\left[ \ell^{(t)}_{i,j^{(t)}}\right]$$
	Then $\rho$ can be chosen in \texttt{HExp3} such that for all $i\in[M]$
	\begin{equation}
		\expec\left[\sum_{t=1}^T\ell^{(t)}_{i^{(t)},j^{(t)}}\right]
		\leq \sum_{t=1}^T \tilde{\ell}^{(t)}_{i}
		+ O(\sqrt{T M\log M})
	\end{equation}
	Moreover, $\rho$ and $\eta$ can be chosen such that for all $i\in[M]$ and $j\in[N]$
	\begin{equation}
		\expec\left[\sum_{t=1}^{T} \ell^{(t)}_{i^{(t)},j^{(t)}}\right]
		\leq \sum_{t=1}^T\ell^{(t)}_{i,j}
		+ O(\sqrt{TM \log M} + \sqrt{T N\log N})
	\end{equation}
\end{lem}


\begin{algorithm}[tb]
   \caption{\texttt{Hierarchical Exp3 (HExp3)}}
   \label{alg:meta-exp2}
   \begin{algorithmic}   
      \STATE {\bfseries Input:} $\eta,\rho\in[0,1]$\\
      \STATE $v^{(1)}_{i}=1$ for $i\in[M]$
   	\STATE $w^{(1)}_{i,j}=1$ for $(i,j)\in[M]\times[N]$
	\FOR{$t=1$ {\bfseries to} $T$}
	   \STATE Set $\x^{(t)} = (1-\eta) \frac{\vt^{(t)}}{\sum_{i\in[M]} v^{(t)}_{i}} + \eta\frac{1}{M}$
	   \STATE Set $\y^{(t)}_{i} = (1-\rho)\frac{\wt^{(t)}_{i}}{\sum_{j\in[N]} w^{(t)}_{i,j}} + \rho\frac{1}{N}$ for $i\in[M]$
	\STATE Draw bandit $i^{(t)}\sim \x^{(t)}$ and arm $j^{(t)}\sim \y^{(t)}_{i^{(t)}}$
	\STATE Pull arm $j^{(t)}$ on bandit $i^{(t)}$
	   \STATE Incur loss $\ell =\ell_{i^{(t)}, j^{(t)}}^{(t)}\in [0,1]$ 
	\STATE Update:
	\begin{align}
		v^{(t+1)}_i & = \begin{cases}
			v^{(t)}_{i}\cdot 
			\exp\big(-\eta \frac{\ell}{M\cdot x_{i}^{(t)}}\big) & \text{if } i=i^{(t)} \\
			v^{(t)}_{i} & \text{else}
		\end{cases}		 
		\\
		w^{(t+1)}_{i,j} & = \begin{cases}
			w^{(t)}_{i,j}\cdot \exp\big(-\rho\frac{\ell}{N\cdot x_i^{(t)}\cdot y_{i,j}^{(t)}}\big) 
			& \text{if }(i,j)=(i^{(t)}, j^{(t)}) \\
			w^{(t)}_{i,j} &\text{else}
		\end{cases}
	\end{align}
   	\ENDFOR
   	\end{algorithmic}
\end{algorithm}


\begin{thm}[No-regret with respect to \actual, \comply\, and individual treatment advice]\label{thm:cexp}\eod
	Let \texttt{Exp3} be the no-regret algorithm used in Algorithm~\ref{alg:hier-exp} for both the bottom and top-level bandits, with suitable choice of learning rate. Then, \texttt{HB} satisfies
	\begin{equation}
		\expec\left[\sum_{t=1}^T\ell^{(t)}_{a^{(t)}}\right]
		\leq \sum_{t=1}^T \tilde{\ell}^{(t)}_{\actual/\comply}
		+ O(\sqrt{T})
	\end{equation}
	where $\tilde{\ell}^{(t)}_{\actual/\comply}$ denotes the expected loss vector of \texttt{Exp3} under the respective protocol on round $t$. 
	Furthermore, the regret against individual treatments $j\in[K]$ is bounded by
	\begin{equation}
		\expec\left[\sum_{t=1}^{T} \ell^{(t)}_{a^{(t)}}\right]
		\leq \sum_{t=1}^T\ell^{(t)}_{j}
		+ O(\sqrt{T K\log K})
	\end{equation}
\end{thm}

\begin{proof}
	Apply Lemma~\ref{lem:meta-exp} to $\texttt{HierarchicalBandit}$.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
